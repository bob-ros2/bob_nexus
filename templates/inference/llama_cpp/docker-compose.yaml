services:
  server:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    # The project name (-p) will be the entity name, so we just name the service 'server'
    container_name: nexus_${NAME}
    restart: unless-stopped
    ports:
      - "${LLAMA_CPP_PORT:-8081}:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    # llama.cpp server flags to auto-download from HF
    command:
      - "-hf"
      - "${HF_REPO:-Qwen/Qwen2.5-0.5B-Instruct-GGUF}"
      - "-hff"
      - "${HF_FILE:-qwen2.5-0.5b-instruct-q4_k_m.gguf}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
      - "-ngl"
      - "99"
    volumes:
      - llama_models:/models

volumes:
  llama_models:
    name: nexus_inference_models
